const express = require('express');
const cors = require('cors');
const dotenv = require('dotenv');
const axios = require('axios');
const FormData = require('form-data');

dotenv.config();

const app = express();
const PORT = process.env.PORT || 3001;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

// Middleware
app.use(cors());
app.use(express.json({ limit: '50mb' }));
app.use(express.urlencoded({ limit: '50mb', extended: true }));

// Health check
app.get('/health', (req, res) => {
  res.json({ status: 'ok', message: 'Rahman\'s backend is running! üëë' });
});

/**
 * POST /api/voice/process
 * Handles voice transcription and parsing
 *
 * Body:
 * - audio: base64 encoded audio data
 * - context: 'calendar' or 'todo'
 */
app.post('/api/voice/process', async (req, res) => {
  try {
    const { audio, context, mimeType, byteLength } = req.body;

    if (!audio || !context) {
      return res.status(400).json({ error: 'Missing audio or context' });
    }

    if (!OPENAI_API_KEY) {
      return res.status(500).json({ error: 'API key not configured' });
    }

    // Decode base64 audio to buffer
    const audioBuffer = Buffer.from(audio, 'base64');
    if (!audioBuffer.length || audioBuffer.length < 512) {
      return res.status(400).json({
        error: 'No usable audio captured. Record for 1-2 seconds and try again.'
      });
    }

    // Step 1: Transcribe audio using Whisper
    console.log(`üé§ Transcribing audio... mime=${mimeType || 'unknown'} bytes=${audioBuffer.length} clientBytes=${byteLength || 'n/a'}`);
    const transcript = await transcribeAudio(audioBuffer, mimeType);

    if (!transcript || transcript.trim().length === 0) {
      return res.status(400).json({ error: 'Could not transcribe audio' });
    }

    console.log(`üìù Transcript: "${transcript}"`);

    // Step 2: Parse transcript with GPT-4.1-mini based on context
    console.log(`ü§ñ Parsing with GPT-4.1-mini (context: ${context})...`);
    const parsed = await parseWithGPT(transcript, context);

    // Fallback for TODO extraction when model returns no tasks for usable speech
    if (context === 'todo') {
      if (!Array.isArray(parsed.tasks)) {
        parsed.tasks = [];
      }
      if (parsed.tasks.length === 0) {
        const fallbackTasks = fallbackTodoTasksFromTranscript(transcript);
        if (fallbackTasks.length > 0) {
          parsed.tasks = fallbackTasks;
          console.log('üõü Applied fallback task extraction');
        }
      }
      console.log(`üßæ Parsed tasks count: ${parsed.tasks.length}`);
    }

    console.log('‚úÖ Processing complete');
    res.json({
      transcript,
      parsed,
      context
    });

  } catch (error) {
    console.error('‚ùå Error:', error.message);
    res.status(500).json({
      error: error.message || 'Processing failed',
      details: process.env.NODE_ENV === 'development' ? error.stack : undefined
    });
  }
});

/**
 * Transcribe audio using OpenAI Whisper API
 */
async function transcribeAudio(audioBuffer, rawMimeType) {
  try {
    const { filename, contentType } = getAudioFileMeta(rawMimeType);
    const form = new FormData();
    form.append('file', audioBuffer, { filename, contentType });
    form.append('model', 'whisper-1');

    const response = await axios.post(
      'https://api.openai.com/v1/audio/transcriptions',
      form,
      {
        headers: {
          ...form.getHeaders(),
          'Authorization': `Bearer ${OPENAI_API_KEY}`
        },
        timeout: 30000
      }
    );

    return response.data.text || '';
  } catch (error) {
    console.error('Whisper API error:', error.response?.data || error.message);
    throw new Error(`Transcription failed: ${error.response?.data?.error?.message || error.message}`);
  }
}

function getAudioFileMeta(rawMimeType) {
  const normalized = (rawMimeType || 'audio/webm').split(';')[0].trim().toLowerCase();
  const extensionByMimeType = {
    'audio/webm': 'webm',
    'audio/mp4': 'mp4',
    'audio/x-m4a': 'm4a',
    'audio/m4a': 'm4a',
    'audio/mpeg': 'mp3',
    'audio/mp3': 'mp3',
    'audio/wav': 'wav',
    'audio/x-wav': 'wav',
    'audio/ogg': 'ogg'
  };

  const extension = extensionByMimeType[normalized] || 'webm';
  return {
    filename: `audio.${extension}`,
    contentType: normalized || 'audio/webm'
  };
}

/**
 * Parse transcript using GPT-4.1-mini
 */
async function parseWithGPT(transcript, context) {
  const today = new Date();
  const todayStr = today.toISOString().split('T')[0];
  const dayName = today.toLocaleDateString('en-US', { weekday: 'long' });

  let systemPrompt;

  if (context === 'calendar') {
    systemPrompt = `You are a calendar assistant for Rahman. Today is ${dayName}, ${todayStr}.
Extract calendar events from user speech. Return ONLY valid JSON, no markdown, no code blocks:
{"events": [{"title": "string", "date": "YYYY-MM-DD", "time": "HH:MM" or null}]}
If user says "tomorrow", "next Monday", etc., calculate the correct date.
If no specific time mentioned, set time to null.
Always return an array, even if empty.`;
  } else {
    systemPrompt = `You are a to-do list assistant for Rahman.
Extract tasks from user speech. Return ONLY valid JSON, no markdown, no code blocks:
{"tasks": [{"title": "string", "priority": "high" or "medium" or "low", "notes": "string or empty"}]}
Infer priority: "urgent", "ASAP", "important" = high. "whenever", "at some point" = low. Default = medium.
Extract extra details as notes.
If transcript has at least one plausible actionable item, return at least one task.
Only return an empty array for pure filler/greeting/noise (e.g. "oh", "um", "hello").`;
  }

  try {
    const response = await axios.post(
      'https://api.openai.com/v1/chat/completions',
      {
        model: 'gpt-4.1-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: transcript }
        ],
        temperature: 0.2,
        max_tokens: 500
      },
      {
        headers: {
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        timeout: 30000
      }
    );

    const content = response.data.choices[0].message.content.trim();

    // Parse JSON, handling potential markdown code fences
    let jsonStr = content;
    if (jsonStr.includes('```')) {
      jsonStr = jsonStr.replace(/```(?:json)?\s*/g, '').trim();
    }

    const parsed = JSON.parse(jsonStr);
    return parsed;
  } catch (error) {
    console.error('GPT API error:', error.response?.data || error.message);
    throw new Error(`Parsing failed: ${error.response?.data?.error?.message || error.message}`);
  }
}

function fallbackTodoTasksFromTranscript(transcript) {
  if (!transcript) {
    return [];
  }

  const cleaned = transcript
    .replace(/[^\w\s'-]/g, ' ')
    .replace(/\s+/g, ' ')
    .trim();

  if (!cleaned) {
    return [];
  }

  const lowered = cleaned.toLowerCase();
  const fillerOnly = new Set(['oh', 'uh', 'um', 'hmm', 'huh', 'hello', 'hi', 'hey', 'test']);
  if (fillerOnly.has(lowered)) {
    return [];
  }

  let title = cleaned
    .replace(/^(add|create|set|make|new)\s+(a\s+)?(task|todo)\s*/i, '')
    .replace(/^(todo|to do)\s*/i, '')
    .replace(/^(remind me to|i need to|need to)\s*/i, '')
    .trim();

  if (!title) {
    title = cleaned;
  }

  const words = title.split(/\s+/).filter(Boolean);
  if (words.length === 0) {
    return [];
  }

  // Avoid creating useless single-token noise tasks.
  if (words.length === 1 && words[0].length < 4) {
    return [];
  }

  if (title.length > 120) {
    title = `${title.slice(0, 117).trim()}...`;
  }

  return [{
    title: title.charAt(0).toUpperCase() + title.slice(1),
    priority: 'medium',
    notes: ''
  }];
}

// 404 handler
app.use((req, res) => {
  res.status(404).json({ error: 'Not found', path: req.path });
});

// Error handler
app.use((err, req, res, next) => {
  console.error('Unhandled error:', err);
  res.status(500).json({ error: 'Internal server error' });
});

// Start server
app.listen(PORT, () => {
  console.log(`\nüëë Rahman's Backend Server\n`);
  console.log(`‚úÖ Running on http://localhost:${PORT}`);
  console.log(`üìù Transcript endpoint: POST /api/voice/process`);
  console.log(`üíö Health check: GET /health\n`);
});
